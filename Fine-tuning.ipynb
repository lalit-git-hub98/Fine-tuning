{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNATLvt4ueDInLzd/qY425K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"e3f94d2248824f0eb1bad74cffebd856":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a92e72a76a054df8af4559b01927cbab","IPY_MODEL_62001d4d5f4e4b6489bd0d7cf64028c7","IPY_MODEL_49dd9a95ccde4cb9aca7ec82d22a258c"],"layout":"IPY_MODEL_36a8da3f0d2f430fb74a20ad807fe334"}},"a92e72a76a054df8af4559b01927cbab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a8286b077264904b199e6d7290a3de4","placeholder":"​","style":"IPY_MODEL_3c4297f12b394b32a9337507bdc1eb82","value":"tokenizer_config.json: 100%"}},"62001d4d5f4e4b6489bd0d7cf64028c7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1fcfae1db804cbeb250e3a3b50a5f48","max":2539,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2547f0ba9e984192b4d28c2e87e614e0","value":2539}},"49dd9a95ccde4cb9aca7ec82d22a258c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a36dda76904a4d33abda42a9c9efe914","placeholder":"​","style":"IPY_MODEL_1f36d11701d44dafa0252f395201d4d0","value":" 2.54k/2.54k [00:00&lt;00:00, 240kB/s]"}},"36a8da3f0d2f430fb74a20ad807fe334":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a8286b077264904b199e6d7290a3de4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c4297f12b394b32a9337507bdc1eb82":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b1fcfae1db804cbeb250e3a3b50a5f48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2547f0ba9e984192b4d28c2e87e614e0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a36dda76904a4d33abda42a9c9efe914":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f36d11701d44dafa0252f395201d4d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c013a9812b8144cea6e69441de6c78ab":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_92ac5043265041468232a658d576ed10","IPY_MODEL_37d653dc581d4b5f91c6ac4cab4c2b0f","IPY_MODEL_c0c26d3a24224f50b5b490ce6d0b1470"],"layout":"IPY_MODEL_887e1c4fce8b4a9e9f6ff415b9e13c45"}},"92ac5043265041468232a658d576ed10":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb94052ad8894f62b46ab320e98c67b6","placeholder":"​","style":"IPY_MODEL_1780287100e34eeaaecf7e149f195baa","value":"spiece.model: 100%"}},"37d653dc581d4b5f91c6ac4cab4c2b0f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_efc81d41b9644ea293bbac21f80ce58e","max":791656,"min":0,"orientation":"horizontal","style":"IPY_MODEL_85c6636fdb704d4588f9752edb6b3dcb","value":791656}},"c0c26d3a24224f50b5b490ce6d0b1470":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_839fd6f79ffc47b0b5e7d9b993e06eb1","placeholder":"​","style":"IPY_MODEL_2ea87a27ea594fb2aa2163617faa0bd9","value":" 792k/792k [00:00&lt;00:00, 33.5MB/s]"}},"887e1c4fce8b4a9e9f6ff415b9e13c45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb94052ad8894f62b46ab320e98c67b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1780287100e34eeaaecf7e149f195baa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"efc81d41b9644ea293bbac21f80ce58e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85c6636fdb704d4588f9752edb6b3dcb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"839fd6f79ffc47b0b5e7d9b993e06eb1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ea87a27ea594fb2aa2163617faa0bd9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8bb3b5c6cd3842ce9a3fea48070d0581":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9f19673650c74f7eb3389eb289411b17","IPY_MODEL_1d3dda790e27405bb28021cd52c89c4d","IPY_MODEL_2ba20bbf6dcf4e0fba734bdefe670209"],"layout":"IPY_MODEL_d5c3d7c316544832a53417c275704b81"}},"9f19673650c74f7eb3389eb289411b17":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b5b2acded3a40ec8b59541f82fcd34f","placeholder":"​","style":"IPY_MODEL_21601b61e9854c8397c8deb38d33d120","value":"tokenizer.json: 100%"}},"1d3dda790e27405bb28021cd52c89c4d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_060042d7fea84a2ea8ba6ce6e1879643","max":2424064,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d924601ec0b6412d9ffaa46f121ecdfb","value":2424064}},"2ba20bbf6dcf4e0fba734bdefe670209":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_22d81c04d5ab4eaca63c07bd6f63eb41","placeholder":"​","style":"IPY_MODEL_f05e08f9d54349fa8e52281913b13657","value":" 2.42M/2.42M [00:00&lt;00:00, 11.0MB/s]"}},"d5c3d7c316544832a53417c275704b81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b5b2acded3a40ec8b59541f82fcd34f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21601b61e9854c8397c8deb38d33d120":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"060042d7fea84a2ea8ba6ce6e1879643":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d924601ec0b6412d9ffaa46f121ecdfb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"22d81c04d5ab4eaca63c07bd6f63eb41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f05e08f9d54349fa8e52281913b13657":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ffec11d853f44e68df49000b67d05a7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_97fa8adeb4b14304bcd22137e1149936","IPY_MODEL_ef52b62fcc7945d6bcb1ef98c1ceb4f2","IPY_MODEL_d58b46aa57894d6198931515c44d93de"],"layout":"IPY_MODEL_75bf95f0ebdd49a5bbca21c17d3c8e76"}},"97fa8adeb4b14304bcd22137e1149936":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_222ef63df5184e268702ad9c6bfe3080","placeholder":"​","style":"IPY_MODEL_24f38a7827a843ecbd30af5144ff78ed","value":"special_tokens_map.json: 100%"}},"ef52b62fcc7945d6bcb1ef98c1ceb4f2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8fedaaf97314451d958683df866cfef8","max":2201,"min":0,"orientation":"horizontal","style":"IPY_MODEL_63ef07b4cac148abaab68b0f74b33e7a","value":2201}},"d58b46aa57894d6198931515c44d93de":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ff4b8f82aec4ca2b10d5d96a083b3aa","placeholder":"​","style":"IPY_MODEL_c43696ef7dee4529a1993e244fcd7150","value":" 2.20k/2.20k [00:00&lt;00:00, 230kB/s]"}},"75bf95f0ebdd49a5bbca21c17d3c8e76":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"222ef63df5184e268702ad9c6bfe3080":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24f38a7827a843ecbd30af5144ff78ed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8fedaaf97314451d958683df866cfef8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63ef07b4cac148abaab68b0f74b33e7a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7ff4b8f82aec4ca2b10d5d96a083b3aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c43696ef7dee4529a1993e244fcd7150":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9dc52fd7f3149048589c442d1c88cac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_21caa558ebd6403abdb29d1b5fad1da4","IPY_MODEL_6b7369c1004344ec8b043f74515d4bbf","IPY_MODEL_ed16a71805cf4dff9087f1e82a359e4a"],"layout":"IPY_MODEL_ecf9566fb32f40b89614bc296788b9d6"}},"21caa558ebd6403abdb29d1b5fad1da4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89aea8ca54af4f32beb3c09814c85773","placeholder":"​","style":"IPY_MODEL_c99bc91790b44548bd9d41b4aa1bb7e4","value":"config.json: 100%"}},"6b7369c1004344ec8b043f74515d4bbf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d321ec615d94e7bbd7a1125da5dc3ed","max":1401,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ee8e8ecb635d4991a10cd798357bae2d","value":1401}},"ed16a71805cf4dff9087f1e82a359e4a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4ae999c2aee4049bbbf5b9c3933729a","placeholder":"​","style":"IPY_MODEL_e2cfb547a5b74d6e89206c8f55e115e7","value":" 1.40k/1.40k [00:00&lt;00:00, 74.3kB/s]"}},"ecf9566fb32f40b89614bc296788b9d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89aea8ca54af4f32beb3c09814c85773":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c99bc91790b44548bd9d41b4aa1bb7e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6d321ec615d94e7bbd7a1125da5dc3ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee8e8ecb635d4991a10cd798357bae2d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f4ae999c2aee4049bbbf5b9c3933729a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2cfb547a5b74d6e89206c8f55e115e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["!pip install transformers datasets pandas numpy torch peft bitsandbytes accelerate matplotlib seaborn scikit-learn tqdm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WcMjkEu7J32g","executionInfo":{"status":"ok","timestamp":1745250871371,"user_tz":240,"elapsed":118116,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}},"outputId":"5222688b-eccb-4d64-d650-8143d88e1bfc"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n","Collecting datasets\n","  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, bitsandbytes\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.2\n","    Uninstalling fsspec-2025.3.2:\n","      Successfully uninstalled fsspec-2025.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed bitsandbytes-0.45.5 datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 xxhash-3.5.0\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"LrfbOS4-I57I","executionInfo":{"status":"ok","timestamp":1745250949729,"user_tz":240,"elapsed":25450,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}}},"outputs":[],"source":["import os\n","import torch\n","import numpy as np\n","import pandas as pd\n","import transformers\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    TrainingArguments,\n","    Trainer,\n","    BitsAndBytesConfig,\n","    DataCollatorForLanguageModeling\n",")\n","from peft import (\n","    get_peft_model,\n","    LoraConfig,\n","    TaskType,\n","    PeftModel,\n","    PeftConfig,\n","    prepare_model_for_kbit_training\n",")\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tqdm import tqdm\n","import gc"]},{"cell_type":"code","source":["print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"Transformers version: {transformers.__version__}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M51EdKlpJa2x","executionInfo":{"status":"ok","timestamp":1745250954397,"user_tz":240,"elapsed":7,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}},"outputId":"6b46cfdd-455f-413a-9979-8872e6519109"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch version: 2.6.0+cu124\n","Transformers version: 4.51.3\n"]}]},{"cell_type":"code","source":["# Set random seeds for reproducibility\n","RANDOM_SEED = 42\n","torch.manual_seed(RANDOM_SEED)\n","np.random.seed(RANDOM_SEED)"],"metadata":{"id":"slR1lj45JeDt","executionInfo":{"status":"ok","timestamp":1745250956275,"user_tz":240,"elapsed":3,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MZ64m1rMJf6D","executionInfo":{"status":"ok","timestamp":1745250958030,"user_tz":240,"elapsed":69,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}},"outputId":"cd0989e0-a840-415b-d925-4c8bf593f266"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"code","source":["# Common configuration parameters\n","MODEL_NAME = \"google/flan-t5-small\"  # Change to your preferred base model\n","MAX_LENGTH = 512\n","BATCH_SIZE = 4\n","EPOCHS = 3\n","LEARNING_RATE = 2e-5\n","SAVE_PATH = \"./fine_tuned_models/\"\n","\n","os.makedirs(SAVE_PATH, exist_ok=True)"],"metadata":{"id":"joBzsz94NCIh","executionInfo":{"status":"ok","timestamp":1745251872508,"user_tz":240,"elapsed":6,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["# Load and prepare dataset\n","def prepare_dataset():\n","    \"\"\"Load and prepare the dataset for fine-tuning.\"\"\"\n","    # Sample data - in a real scenario, load your own dataset\n","    # Here we'll use a small subset of the OpenAssistant Conversations Dataset\n","    dataset = load_dataset(\"OpenAssistant/oasst1\", split=\"train\")\n","    dataset = dataset.filter(lambda x: x[\"lang\"] == \"en\").select(range(1000))\n","\n","    # Format data for instruction tuning with the following structure:\n","    # 1. System message (optional)\n","    # 2. User query\n","    # 3. Assistant response\n","    formatted_data = []\n","\n","    for item in dataset:\n","        if \"text\" in item and item[\"text\"].strip():\n","            # Simple formatting example\n","            formatted_text = f\"### Instruction: {item['text']}\\n\\n### Response: This is a helpful response.\"\n","            formatted_data.append({\"text\": formatted_text})\n","\n","    # Split into train and validation sets\n","    train_data, val_data = train_test_split(formatted_data, test_size=0.1, random_state=RANDOM_SEED)\n","\n","    return {\"train\": train_data, \"validation\": val_data}"],"metadata":{"id":"vTSvwPCpBTUy","executionInfo":{"status":"ok","timestamp":1745251358655,"user_tz":240,"elapsed":3,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# Tokenize dataset\n","def tokenize_dataset(dataset, tokenizer):\n","    \"\"\"Tokenize the dataset for training.\"\"\"\n","\n","    def tokenize_function(examples):\n","        return tokenizer(\n","            examples[\"text\"],\n","            padding=\"max_length\",\n","            truncation=True,\n","            max_length=MAX_LENGTH\n","        )\n","\n","    tokenized_dataset = {}\n","    for split, data in dataset.items():\n","        texts = [item[\"text\"] for item in data]\n","        tokenized_texts = tokenize_function({\"text\": texts})\n","\n","        # Convert to the expected format for Trainer\n","        tokenized_dataset[split] = {\n","            \"input_ids\": tokenized_texts[\"input_ids\"],\n","            \"attention_mask\": tokenized_texts[\"attention_mask\"]\n","        }\n","\n","    return tokenized_dataset"],"metadata":{"id":"sik5iGQdBTvz","executionInfo":{"status":"ok","timestamp":1745251359709,"user_tz":240,"elapsed":13,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# Load tokenizer\n","def load_tokenizer(model_name):\n","    \"\"\"Load and configure the tokenizer.\"\"\"\n","    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n","    if tokenizer.pad_token is None:\n","        tokenizer.pad_token = tokenizer.eos_token\n","    return tokenizer"],"metadata":{"id":"QH7lKkdDBUHS","executionInfo":{"status":"ok","timestamp":1745251360541,"user_tz":240,"elapsed":4,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# Helper function to calculate model size\n","def get_model_size(model):\n","    \"\"\"Calculate and return the model size in GB.\"\"\"\n","    param_size = 0\n","    for param in model.parameters():\n","        param_size += param.nelement() * param.element_size()\n","    buffer_size = 0\n","    for buffer in model.buffers():\n","        buffer_size += buffer.nelement() * buffer.element_size()\n","\n","    size_all_mb = (param_size + buffer_size) / 1024**2\n","    return size_all_mb / 1024  # Convert MB to GB"],"metadata":{"id":"vYqC-d2ZBUaj","executionInfo":{"status":"ok","timestamp":1745251361276,"user_tz":240,"elapsed":31,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# Helper function to free GPU memory\n","def free_memory():\n","    \"\"\"Free GPU memory.\"\"\"\n","    gc.collect()\n","    torch.cuda.empty_cache()"],"metadata":{"id":"xumXtxhLBUy9","executionInfo":{"status":"ok","timestamp":1745251361916,"user_tz":240,"elapsed":4,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["# 1. Full-Parameter Fine-tuning"],"metadata":{"id":"JiF51JDhBw1L"}},{"cell_type":"code","source":["def full_parameter_finetuning():\n","    \"\"\"Implement full-parameter fine-tuning.\"\"\"\n","    print(\"\\n=== Starting Full-Parameter Fine-tuning ===\")\n","\n","    # Load tokenizer and prepare dataset\n","    tokenizer = load_tokenizer(MODEL_NAME)\n","    dataset = prepare_dataset()\n","    tokenized_dataset = tokenize_dataset(dataset, tokenizer)\n","\n","    # Data collator for language modeling\n","    data_collator = DataCollatorForLanguageModeling(\n","        tokenizer=tokenizer,\n","        mlm=False  # Not using masked language modeling\n","    )\n","\n","    # Load model\n","    print(\"Loading the model (this might take a while)...\")\n","    model = AutoModelForCausalLM.from_pretrained(\n","        MODEL_NAME,\n","        torch_dtype=torch.float16,\n","        device_map=\"auto\"\n","    )\n","\n","    print(f\"Model loaded successfully. Size: {get_model_size(model):.2f} GB\")\n","\n","    # Set up training arguments\n","    training_args = TrainingArguments(\n","        output_dir=f\"{SAVE_PATH}/full_param_finetuned\",\n","        per_device_train_batch_size=BATCH_SIZE,\n","        per_device_eval_batch_size=BATCH_SIZE,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=100,\n","        logging_dir=f\"{SAVE_PATH}/logs\",\n","        logging_steps=10,\n","        learning_rate=LEARNING_RATE,\n","        num_train_epochs=EPOCHS,\n","        weight_decay=0.01,\n","        fp16=True,\n","        save_strategy=\"epoch\",\n","        save_total_limit=2,\n","        load_best_model_at_end=True,\n","        report_to=\"tensorboard\"\n","    )\n","\n","    # Create Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=list(tokenized_dataset[\"train\"].values()),\n","        eval_dataset=list(tokenized_dataset[\"validation\"].values()),\n","        data_collator=data_collator,\n","        tokenizer=tokenizer\n","    )\n","\n","    # Train model\n","    print(\"Starting training...\")\n","    trainer.train()\n","\n","    # Save model\n","    print(\"Saving model...\")\n","    trainer.save_model(f\"{SAVE_PATH}/full_param_finetuned_final\")\n","    tokenizer.save_pretrained(f\"{SAVE_PATH}/full_param_finetuned_final\")\n","\n","    print(\"Full-parameter fine-tuning completed!\")\n","\n","    # Free memory\n","    del model, trainer\n","    free_memory()"],"metadata":{"id":"hZCE92DbBVTg","executionInfo":{"status":"ok","timestamp":1745251363327,"user_tz":240,"elapsed":34,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["# 2. Partial Fine-tuning (Freezing certain layers)"],"metadata":{"id":"OJRClwnkB8zM"}},{"cell_type":"code","source":["def partial_finetuning():\n","    \"\"\"Implement partial fine-tuning by freezing earlier layers.\"\"\"\n","    print(\"\\n=== Starting Partial Fine-tuning ===\")\n","\n","    # Load tokenizer and prepare dataset\n","    tokenizer = load_tokenizer(MODEL_NAME)\n","    dataset = prepare_dataset()\n","    tokenized_dataset = tokenize_dataset(dataset, tokenizer)\n","\n","    # Data collator\n","    data_collator = DataCollatorForLanguageModeling(\n","        tokenizer=tokenizer,\n","        mlm=False\n","    )\n","\n","    # Load model\n","    print(\"Loading the model...\")\n","    model = AutoModelForCausalLM.from_pretrained(\n","        MODEL_NAME,\n","        torch_dtype=torch.float16,\n","        device_map=\"auto\"\n","    )\n","\n","    # Freeze the bottom layers (typically the first 70-80% for partial fine-tuning)\n","    print(\"Freezing bottom layers...\")\n","\n","    # For transformer-based models\n","    modules = list(model.named_modules())\n","\n","    # Count the transformer blocks/layers\n","    transformer_blocks = [name for name, _ in modules if \"block\" in name or \"layer\" in name]\n","    num_layers = len(transformer_blocks)\n","    num_frozen = int(num_layers * 0.7)  # Freeze 70% of the layers\n","\n","    # Freeze specific layers\n","    for name, param in model.named_parameters():\n","        layer_num = None\n","        for i in range(num_layers):\n","            if f\"block.{i}.\" in name or f\"layer.{i}.\" in name:\n","                layer_num = i\n","                break\n","\n","        # Freeze parameters in the bottom layers\n","        if layer_num is not None and layer_num < num_frozen:\n","            param.requires_grad = False\n","\n","    # Count trainable parameters\n","    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    total_params = sum(p.numel() for p in model.parameters())\n","    print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params:.2%} of total)\")\n","\n","    # Set up training arguments\n","    training_args = TrainingArguments(\n","        output_dir=f\"{SAVE_PATH}/partial_finetuned\",\n","        per_device_train_batch_size=BATCH_SIZE,\n","        per_device_eval_batch_size=BATCH_SIZE,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=100,\n","        logging_dir=f\"{SAVE_PATH}/logs\",\n","        logging_steps=10,\n","        learning_rate=LEARNING_RATE,\n","        num_train_epochs=EPOCHS,\n","        weight_decay=0.01,\n","        fp16=True,\n","        save_strategy=\"epoch\",\n","        save_total_limit=2,\n","        load_best_model_at_end=True,\n","        report_to=\"tensorboard\"\n","    )\n","\n","    # Create Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=list(tokenized_dataset[\"train\"].values()),\n","        eval_dataset=list(tokenized_dataset[\"validation\"].values()),\n","        data_collator=data_collator,\n","        tokenizer=tokenizer\n","    )\n","\n","    # Train model\n","    print(\"Starting training...\")\n","    trainer.train()\n","\n","    # Save model\n","    print(\"Saving model...\")\n","    trainer.save_model(f\"{SAVE_PATH}/partial_finetuned_final\")\n","    tokenizer.save_pretrained(f\"{SAVE_PATH}/partial_finetuned_final\")\n","\n","    print(\"Partial fine-tuning completed!\")\n","\n","    # Free memory\n","    del model, trainer\n","    free_memory()"],"metadata":{"id":"wpDipwSFBVon","executionInfo":{"status":"ok","timestamp":1745251364790,"user_tz":240,"elapsed":5,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["# 3. LoRA (Low-Rank Adaptation) Fine-tuning"],"metadata":{"id":"pY7OoIgYCDhs"}},{"cell_type":"code","source":["def lora_finetuning():\n","    \"\"\"Implement LoRA fine-tuning.\"\"\"\n","    print(\"\\n=== Starting LoRA Fine-tuning ===\")\n","\n","    # Load tokenizer and prepare dataset\n","    tokenizer = load_tokenizer(MODEL_NAME)\n","    dataset = prepare_dataset()\n","    tokenized_dataset = tokenize_dataset(dataset, tokenizer)\n","\n","    # Data collator\n","    data_collator = DataCollatorForLanguageModeling(\n","        tokenizer=tokenizer,\n","        mlm=False\n","    )\n","\n","    # Load model in 16-bit precision\n","    print(\"Loading the base model...\")\n","    model = AutoModelForCausalLM.from_pretrained(\n","        MODEL_NAME,\n","        torch_dtype=torch.float16,\n","        device_map=\"auto\"\n","    )\n","\n","    # Define LoRA configuration\n","    peft_config = LoraConfig(\n","        task_type=TaskType.CAUSAL_LM,\n","        inference_mode=False,\n","        r=8,                       # Rank of update matrices\n","        lora_alpha=32,             # Alpha parameter for LoRA scaling\n","        lora_dropout=0.1,          # Dropout probability for LoRA layers\n","        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # Which modules to apply LoRA to\n","    )\n","\n","    # Get PEFT model\n","    print(\"Applying LoRA adapters...\")\n","    model = get_peft_model(model, peft_config)\n","    model.print_trainable_parameters()  # Print trainable parameters info\n","\n","    # Set up training arguments\n","    training_args = TrainingArguments(\n","        output_dir=f\"{SAVE_PATH}/lora_finetuned\",\n","        per_device_train_batch_size=BATCH_SIZE,\n","        per_device_eval_batch_size=BATCH_SIZE,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=100,\n","        logging_dir=f\"{SAVE_PATH}/logs\",\n","        logging_steps=10,\n","        learning_rate=LEARNING_RATE,\n","        num_train_epochs=EPOCHS,\n","        weight_decay=0.01,\n","        fp16=True,\n","        save_strategy=\"epoch\",\n","        save_total_limit=2,\n","        load_best_model_at_end=True,\n","        report_to=\"tensorboard\"\n","    )\n","\n","    # Create Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=list(tokenized_dataset[\"train\"].values()),\n","        eval_dataset=list(tokenized_dataset[\"validation\"].values()),\n","        data_collator=data_collator,\n","        tokenizer=tokenizer\n","    )\n","\n","    # Train model\n","    print(\"Starting training...\")\n","    trainer.train()\n","\n","    # Save model\n","    print(\"Saving LoRA adapters...\")\n","    model.save_pretrained(f\"{SAVE_PATH}/lora_finetuned_final\")\n","    tokenizer.save_pretrained(f\"{SAVE_PATH}/lora_finetuned_final\")\n","\n","    print(\"LoRA fine-tuning completed!\")\n","\n","    # Free memory\n","    del model, trainer\n","    free_memory()"],"metadata":{"id":"lxouiNbNBV64","executionInfo":{"status":"ok","timestamp":1745251366291,"user_tz":240,"elapsed":13,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["# 4. QLoRA (Quantized LoRA) Fine-tuning"],"metadata":{"id":"RswbgJMZCM3j"}},{"cell_type":"code","source":["def qlora_finetuning():\n","    \"\"\"Implement QLoRA fine-tuning.\"\"\"\n","    print(\"\\n=== Starting QLoRA Fine-tuning ===\")\n","\n","    # Load tokenizer and prepare dataset\n","    tokenizer = load_tokenizer(MODEL_NAME)\n","    dataset = prepare_dataset()\n","    tokenized_dataset = tokenize_dataset(dataset, tokenizer)\n","\n","    # Data collator\n","    data_collator = DataCollatorForLanguageModeling(\n","        tokenizer=tokenizer,\n","        mlm=False\n","    )\n","\n","    # Configure quantization\n","    print(\"Configuring 4-bit quantization...\")\n","    bnb_config = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_use_double_quant=True,\n","        bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_compute_dtype=torch.float16\n","    )\n","\n","    # Load model with quantization\n","    print(\"Loading the quantized model...\")\n","    model = AutoModelForCausalLM.from_pretrained(\n","        MODEL_NAME,\n","        quantization_config=bnb_config,\n","        device_map=\"auto\"\n","    )\n","\n","    # Prepare model for k-bit training\n","    model = prepare_model_for_kbit_training(model)\n","\n","    # Define LoRA configuration for QLoRA\n","    peft_config = LoraConfig(\n","        task_type=TaskType.CAUSAL_LM,\n","        inference_mode=False,\n","        r=8,\n","        lora_alpha=32,\n","        lora_dropout=0.1,\n","        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n","    )\n","\n","    # Get PEFT model\n","    print(\"Applying LoRA adapters to quantized model...\")\n","    model = get_peft_model(model, peft_config)\n","    model.print_trainable_parameters()\n","\n","    # Set up training arguments\n","    training_args = TrainingArguments(\n","        output_dir=f\"{SAVE_PATH}/qlora_finetuned\",\n","        per_device_train_batch_size=BATCH_SIZE,\n","        per_device_eval_batch_size=BATCH_SIZE,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=100,\n","        logging_dir=f\"{SAVE_PATH}/logs\",\n","        logging_steps=10,\n","        learning_rate=LEARNING_RATE,\n","        num_train_epochs=EPOCHS,\n","        weight_decay=0.01,\n","        save_strategy=\"epoch\",\n","        save_total_limit=2,\n","        load_best_model_at_end=True,\n","        report_to=\"tensorboard\"\n","    )\n","\n","    # Create Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=list(tokenized_dataset[\"train\"].values()),\n","        eval_dataset=list(tokenized_dataset[\"validation\"].values()),\n","        data_collator=data_collator,\n","        tokenizer=tokenizer\n","    )\n","\n","    # Train model\n","    print(\"Starting training...\")\n","    trainer.train()\n","\n","    # Save model\n","    print(\"Saving QLoRA adapters...\")\n","    model.save_pretrained(f\"{SAVE_PATH}/qlora_finetuned_final\")\n","    tokenizer.save_pretrained(f\"{SAVE_PATH}/qlora_finetuned_final\")\n","\n","    print(\"QLoRA fine-tuning completed!\")\n","\n","    # Free memory\n","    del model, trainer\n","    free_memory()"],"metadata":{"id":"JS4wWqieBWN3","executionInfo":{"status":"ok","timestamp":1745251367786,"user_tz":240,"elapsed":5,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["# 5. PEFT (Parameter-Efficient Fine-Tuning) with different methods"],"metadata":{"id":"1xCdbGc1CTqS"}},{"cell_type":"code","source":["def peft_finetuning():\n","    \"\"\"Implement PEFT fine-tuning with different methods.\"\"\"\n","    print(\"\\n=== Starting PEFT Fine-tuning (Prefix Tuning) ===\")\n","\n","    # Load tokenizer and prepare dataset\n","    tokenizer = load_tokenizer(MODEL_NAME)\n","    dataset = prepare_dataset()\n","    tokenized_dataset = tokenize_dataset(dataset, tokenizer)\n","\n","    # Data collator\n","    data_collator = DataCollatorForLanguageModeling(\n","        tokenizer=tokenizer,\n","        mlm=False\n","    )\n","\n","    # Load model\n","    print(\"Loading the model...\")\n","    model = AutoModelForCausalLM.from_pretrained(\n","        MODEL_NAME,\n","        torch_dtype=torch.float16,\n","        device_map=\"auto\"\n","    )\n","\n","    # Define PEFT configuration for Prefix Tuning\n","    from peft import PrefixTuningConfig\n","\n","    peft_config = PrefixTuningConfig(\n","        task_type=TaskType.CAUSAL_LM,\n","        inference_mode=False,\n","        num_virtual_tokens=20,  # Number of virtual tokens to use\n","        prefix_projection=True,  # Whether to use a projection layer\n","    )\n","\n","    # Get PEFT model\n","    print(\"Applying Prefix Tuning...\")\n","    model = get_peft_model(model, peft_config)\n","    model.print_trainable_parameters()\n","\n","    # Set up training arguments\n","    training_args = TrainingArguments(\n","        output_dir=f\"{SAVE_PATH}/peft_prefix_finetuned\",\n","        per_device_train_batch_size=BATCH_SIZE,\n","        per_device_eval_batch_size=BATCH_SIZE,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=100,\n","        logging_dir=f\"{SAVE_PATH}/logs\",\n","        logging_steps=10,\n","        learning_rate=LEARNING_RATE,\n","        num_train_epochs=EPOCHS,\n","        weight_decay=0.01,\n","        fp16=True,\n","        save_strategy=\"epoch\",\n","        save_total_limit=2,\n","        load_best_model_at_end=True,\n","        report_to=\"tensorboard\"\n","    )\n","\n","    # Create Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=list(tokenized_dataset[\"train\"].values()),\n","        eval_dataset=list(tokenized_dataset[\"validation\"].values()),\n","        data_collator=data_collator,\n","        tokenizer=tokenizer\n","    )\n","\n","    # Train model\n","    print(\"Starting training...\")\n","    trainer.train()\n","\n","    # Save model\n","    print(\"Saving PEFT adapters...\")\n","    model.save_pretrained(f\"{SAVE_PATH}/peft_prefix_finetuned_final\")\n","    tokenizer.save_pretrained(f\"{SAVE_PATH}/peft_prefix_finetuned_final\")\n","\n","    print(\"PEFT (Prefix Tuning) fine-tuning completed!\")\n","\n","    # Free memory\n","    del model, trainer\n","    free_memory()"],"metadata":{"id":"pl3TN7DdBWvL","executionInfo":{"status":"ok","timestamp":1745251369473,"user_tz":240,"elapsed":6,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["# Function to load and use fine-tuned models\n","def load_and_use_model(model_type):\n","    \"\"\"Load and demonstrate usage of fine-tuned models.\"\"\"\n","    print(f\"\\n=== Loading and Using {model_type} Model ===\")\n","\n","    # Load tokenizer\n","    tokenizer = AutoTokenizer.from_pretrained(f\"{SAVE_PATH}/{model_type}_finetuned_final\")\n","\n","    # Load model based on type\n","    if model_type in [\"lora\", \"qlora\", \"peft_prefix\"]:\n","        # For PEFT models (LoRA, QLoRA, Prefix Tuning)\n","        # First load the base model\n","        base_model = AutoModelForCausalLM.from_pretrained(\n","            MODEL_NAME,\n","            torch_dtype=torch.float16,\n","            device_map=\"auto\"\n","        )\n","\n","        # Then load the PEFT adapters\n","        model = PeftModel.from_pretrained(\n","            base_model,\n","            f\"{SAVE_PATH}/{model_type}_finetuned_final\",\n","            device_map=\"auto\"\n","        )\n","    else:\n","        # For full-parameter or partial fine-tuning\n","        model = AutoModelForCausalLM.from_pretrained(\n","            f\"{SAVE_PATH}/{model_type}_finetuned_final\",\n","            torch_dtype=torch.float16,\n","            device_map=\"auto\"\n","        )\n","\n","    # Example usage\n","    test_prompt = \"### Instruction: Explain how neural networks work.\\n\\n### Response:\"\n","\n","    inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n","    outputs = model.generate(\n","        input_ids=inputs[\"input_ids\"],\n","        attention_mask=inputs[\"attention_mask\"],\n","        max_length=256,\n","        temperature=0.7,\n","        top_p=0.9,\n","        do_sample=True\n","    )\n","\n","    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    print(f\"Model response to test prompt:\\n{response}\")\n","\n","    # Free memory\n","    del model\n","    free_memory()"],"metadata":{"id":"nQEOvyhfBXKy","executionInfo":{"status":"ok","timestamp":1745251370770,"user_tz":240,"elapsed":3,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["# Compare fine-tuning methods\n","def compare_methods():\n","    \"\"\"Compare different fine-tuning methods.\"\"\"\n","    print(\"\\n=== Comparing Fine-tuning Methods ===\")\n","\n","    # Define metrics to compare\n","    methods = [\"Full-Parameter\", \"Partial\", \"LoRA\", \"QLoRA\", \"PEFT (Prefix)\"]\n","\n","    # Example metrics (in a real scenario, use actual measurements)\n","    training_time = [240, 180, 50, 40, 45]  # minutes\n","    memory_usage = [24, 18, 8, 4, 6]  # GB\n","    parameter_efficiency = [0, 30, 99.9, 99.9, 99.5]  # % reduction in trainable parameters\n","    inference_performance = [100, 100, 95, 90, 93]  # % relative to full fine-tuning\n","\n","    # Create comparison plots\n","    plt.figure(figsize=(15, 10))\n","\n","    # Training time\n","    plt.subplot(2, 2, 1)\n","    plt.bar(methods, training_time, color='skyblue')\n","    plt.title('Training Time (minutes)')\n","    plt.xticks(rotation=45)\n","\n","    # Memory usage\n","    plt.subplot(2, 2, 2)\n","    plt.bar(methods, memory_usage, color='lightgreen')\n","    plt.title('Memory Usage (GB)')\n","    plt.xticks(rotation=45)\n","\n","    # Parameter efficiency\n","    plt.subplot(2, 2, 3)\n","    plt.bar(methods, parameter_efficiency, color='salmon')\n","    plt.title('Parameter Efficiency (% reduction)')\n","    plt.xticks(rotation=45)\n","\n","    # Inference performance\n","    plt.subplot(2, 2, 4)\n","    plt.bar(methods, inference_performance, color='purple')\n","    plt.title('Inference Performance (% relative)')\n","    plt.xticks(rotation=45)\n","\n","    plt.tight_layout()\n","    plt.savefig(f\"{SAVE_PATH}/comparison_chart.png\")\n","    plt.close()\n","\n","    # Create a comparison table\n","    comparison_data = {\n","        'Method': methods,\n","        'Training Time (min)': training_time,\n","        'Memory Usage (GB)': memory_usage,\n","        'Parameter Efficiency (%)': parameter_efficiency,\n","        'Inference Performance (%)': inference_performance\n","    }\n","\n","    comparison_df = pd.DataFrame(comparison_data)\n","    print(\"\\nComparison of Fine-tuning Methods:\")\n","    print(comparison_df.to_string(index=False))\n","\n","    # Save comparison table\n","    comparison_df.to_csv(f\"{SAVE_PATH}/comparison_table.csv\", index=False)\n","\n","    print(f\"\\nComparison chart saved to {SAVE_PATH}/comparison_chart.png\")\n","    print(f\"Comparison table saved to {SAVE_PATH}/comparison_table.csv\")"],"metadata":{"id":"NJvHS8arByWB","executionInfo":{"status":"ok","timestamp":1745251371803,"user_tz":240,"elapsed":6,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["# Main function to run all implementations\n","def main():\n","    \"\"\"Run all fine-tuning implementations.\"\"\"\n","    print(\"Starting LLM Fine-tuning Methods Implementation\")\n","\n","    # Uncomment the methods you want to run\n","    full_parameter_finetuning()\n","    partial_finetuning()\n","    lora_finetuning()\n","    qlora_finetuning()\n","    peft_finetuning()\n","\n","    # Load and use models\n","    # load_and_use_model(\"full_param\")\n","    # load_and_use_model(\"partial\")\n","    # load_and_use_model(\"lora\")\n","    # load_and_use_model(\"qlora\")\n","    # load_and_use_model(\"peft_prefix\")\n","\n","    # Compare methods\n","    compare_methods()\n","\n","    print(\"\\nAll implementations completed!\")"],"metadata":{"id":"GTPg0SE9ByxE","executionInfo":{"status":"ok","timestamp":1745251373173,"user_tz":240,"elapsed":16,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":814,"referenced_widgets":["e3f94d2248824f0eb1bad74cffebd856","a92e72a76a054df8af4559b01927cbab","62001d4d5f4e4b6489bd0d7cf64028c7","49dd9a95ccde4cb9aca7ec82d22a258c","36a8da3f0d2f430fb74a20ad807fe334","3a8286b077264904b199e6d7290a3de4","3c4297f12b394b32a9337507bdc1eb82","b1fcfae1db804cbeb250e3a3b50a5f48","2547f0ba9e984192b4d28c2e87e614e0","a36dda76904a4d33abda42a9c9efe914","1f36d11701d44dafa0252f395201d4d0","c013a9812b8144cea6e69441de6c78ab","92ac5043265041468232a658d576ed10","37d653dc581d4b5f91c6ac4cab4c2b0f","c0c26d3a24224f50b5b490ce6d0b1470","887e1c4fce8b4a9e9f6ff415b9e13c45","cb94052ad8894f62b46ab320e98c67b6","1780287100e34eeaaecf7e149f195baa","efc81d41b9644ea293bbac21f80ce58e","85c6636fdb704d4588f9752edb6b3dcb","839fd6f79ffc47b0b5e7d9b993e06eb1","2ea87a27ea594fb2aa2163617faa0bd9","8bb3b5c6cd3842ce9a3fea48070d0581","9f19673650c74f7eb3389eb289411b17","1d3dda790e27405bb28021cd52c89c4d","2ba20bbf6dcf4e0fba734bdefe670209","d5c3d7c316544832a53417c275704b81","9b5b2acded3a40ec8b59541f82fcd34f","21601b61e9854c8397c8deb38d33d120","060042d7fea84a2ea8ba6ce6e1879643","d924601ec0b6412d9ffaa46f121ecdfb","22d81c04d5ab4eaca63c07bd6f63eb41","f05e08f9d54349fa8e52281913b13657","7ffec11d853f44e68df49000b67d05a7","97fa8adeb4b14304bcd22137e1149936","ef52b62fcc7945d6bcb1ef98c1ceb4f2","d58b46aa57894d6198931515c44d93de","75bf95f0ebdd49a5bbca21c17d3c8e76","222ef63df5184e268702ad9c6bfe3080","24f38a7827a843ecbd30af5144ff78ed","8fedaaf97314451d958683df866cfef8","63ef07b4cac148abaab68b0f74b33e7a","7ff4b8f82aec4ca2b10d5d96a083b3aa","c43696ef7dee4529a1993e244fcd7150","c9dc52fd7f3149048589c442d1c88cac","21caa558ebd6403abdb29d1b5fad1da4","6b7369c1004344ec8b043f74515d4bbf","ed16a71805cf4dff9087f1e82a359e4a","ecf9566fb32f40b89614bc296788b9d6","89aea8ca54af4f32beb3c09814c85773","c99bc91790b44548bd9d41b4aa1bb7e4","6d321ec615d94e7bbd7a1125da5dc3ed","ee8e8ecb635d4991a10cd798357bae2d","f4ae999c2aee4049bbbf5b9c3933729a","e2cfb547a5b74d6e89206c8f55e115e7"]},"id":"P8JEG87rBzEm","executionInfo":{"status":"error","timestamp":1745251889364,"user_tz":240,"elapsed":8635,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}},"outputId":"ca2a924f-b84d-415b-f096-415f10db9f86"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting LLM Fine-tuning Methods Implementation\n","\n","=== Starting Full-Parameter Fine-tuning ===\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3f94d2248824f0eb1bad74cffebd856"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"]},{"output_type":"display_data","data":{"text/plain":["spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c013a9812b8144cea6e69441de6c78ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bb3b5c6cd3842ce9a3fea48070d0581"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ffec11d853f44e68df49000b67d05a7"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loading the model (this might take a while)...\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9dc52fd7f3149048589c442d1c88cac"}},"metadata":{}},{"output_type":"error","ename":"ValueError","evalue":"Unrecognized configuration class <class 'transformers.models.t5.configuration_t5.T5Config'> for this kind of AutoModel: AutoModelForCausalLM.\nModel type should be one of AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, DeepseekV3Config, DiffLlamaConfig, ElectraConfig, Emu3Config, ErnieConfig, FalconConfig, FalconMambaConfig, FuyuConfig, GemmaConfig, Gemma2Config, Gemma3Config, Gemma3TextConfig, GitConfig, GlmConfig, Glm4Config, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeSharedConfig, HeliumConfig, JambaConfig, JetMoeConfig, LlamaConfig, Llama4Config, Llama4TextConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MllamaConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, Phi4MultimodalConfig, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, ZambaConfig, Zamba2Config.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-972361fa1b80>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-34-c75effe91cfb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Uncomment the methods you want to run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mfull_parameter_finetuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mpartial_finetuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlora_finetuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-27-ef5318abbbbb>\u001b[0m in \u001b[0;36mfull_parameter_finetuning\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading the model (this might take a while)...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             )\n\u001b[0;32m--> 574\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    575\u001b[0m             \u001b[0;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0;34mf\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers.models.t5.configuration_t5.T5Config'> for this kind of AutoModel: AutoModelForCausalLM.\nModel type should be one of AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, DeepseekV3Config, DiffLlamaConfig, ElectraConfig, Emu3Config, ErnieConfig, FalconConfig, FalconMambaConfig, FuyuConfig, GemmaConfig, Gemma2Config, Gemma3Config, Gemma3TextConfig, GitConfig, GlmConfig, Glm4Config, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeSharedConfig, HeliumConfig, JambaConfig, JetMoeConfig, LlamaConfig, Llama4Config, Llama4TextConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MllamaConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, Phi4MultimodalConfig, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, StableLmConfig, Starcode..."]}]},{"cell_type":"code","source":[],"metadata":{"id":"zbLXi7d3EfQ4"},"execution_count":null,"outputs":[]}]}